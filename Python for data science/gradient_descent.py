# -*- coding: utf-8 -*-
"""Gradient descent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hV3I3vzLxWy9V91OwLY3dXXsG_e3JiKb

PRE-REQUISITES
"""

import numpy as np

"""Gradient Descent is an optimization technique that helps models determine the optimal parameter values.
 It adjusts weights and biases by minimizing the cost function.

# IMPELEMENTATION
"""

learning_rate = 0.001
def gradient_descent(x,y):
  m_current = b_current = 0 # starting from 0
  epochs = 100
  n = len(x)
  for i in range(epochs):
    y_predicted = m_current*x + b_current
    cost = 1/n * sum((y - y_predicted)**2)
    # get the m and b derivatives
    m_derivative = (2/n) * sum(x*(y-y_predicted))
    b_derivative = (2/n) + sum(y-y_predicted)
    # adjust the m_current with the m_derivative and the learing rate
    m_current = m_current - learning_rate*m_derivative
    b_current = b_current - learning_rate*b_derivative

    print("m {} , b {} , iterations {} , cost {}".format(m_current,b_current,epochs ,cost))

# function testing
x = np.array([1,2,3,4,5])
y = np.array([3,4,5,6,7])

gradient_descent(x,y)